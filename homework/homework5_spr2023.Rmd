---
title: "Homework 5"
author: "Your Name Here"
subtitle: "Spring 2023"
header-includes:
    - \usepackage{amsmath}
    - \usepackage{amsthm}
output: pdf_document
---

```{r setup, include=FALSE}
#Use this code chunk to include libraries, and set global options.

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
options(pillar.sigfig=6)

```

### Instructions

- This homework is due in Gradescope on Wednesday May 10 by midnight PST.

- Please answer the following questions in the order in which they are posed. 

- Don't forget to knit the document frequently to make sure there are no compilation errors. 

- When you are done, download the PDF file as instructed in section and submit it in Gradescope. 

* * *

### Exercises

1. (Twins) Suppose that in a population of twins consisting only of the two most common biological sexes\footnote{see \href{https://www.joshuakennon.com/the-six-common-biological-sexes-in-humans/}{here} for the six variations that are possible}, males (those with XY chromosomes) and females (those with XX chromosomes) are equally likely and that the probability that the twins are identical is $\alpha_0$. If the twins are not identical, their biological sexes are independently determined. If they are identical, their biological sex is obviously the same.
	
	a. Denote
	\begin{align*}
	\pi_1 &= P(MM),\\
	\pi_2 &= P(FF), \\
	\pi_3 &= P(MF).
	\end{align*}
	  where $\mbox{MM}$ denotes the event that both are male, $\mbox{FF}$ the event that both are female and $\mbox{MF}$ is the event that one of the twins is female and the other is male.
	
  	Then, using the rules of probability, we can say
	
	$$P(MM) = P(FF) = (1+\alpha_0)/4$$ and $$P(MF) = (1-\alpha_0)/2.$$
	 
	 Prove this result for $P(MM)$. Be sure to show and justify your steps. \textbf{Since I am basically setting up the problem formulation for you, we are going to assess mastery of the concepts.}
	
	Hint: Label the members of a pair of twins as 1 and 2. Let $T_1$ denote the event that twin 1 is M, $T_2$ is the event that twin 2 is M. And let $I$ denote the event that the twins are identical. Then the event "MM" is the union of two disjoint events: 
	$$MM = (T_1 \cap T_2 \cap I ) \cup (T_1 \cap T_2 \cap I^c)$$ 
	
	\emph{Review chapters 2 and 4 if you need a reminder}
	
	
	b. Let $\left\{X_1,X_2,X_3\right\}$ denote the number (out of $n$ twins) of $\mbox{MM}$, $\mbox{FF}$ and $\mbox{MF}$. Then a reasonable model is
	$$\langle X_1,X_2,X_3 \rangle \sim Multinom(n, \pi=(\pi_1,\pi_2,\pi_3) )$$
	where $\pi_1$, $\pi_2$ and $\pi_3$ are functions of $\alpha_0$ as described earlier.
	
	  Based on observing $x_1$ MM twins, $x_2$ FF twins, and $x_3$ MF twins, show that the maximum likelihood estimate of $\alpha_0$ is
\begin{eqnarray*}
\widehat{\alpha}_0^{mle} &=& (x_1+x_2-x_3)/n.
\end{eqnarray*}
	  
	  You do not need to verify the second order condition.
	
	c. Is $\widehat{\alpha}_0$ an unbiased estimator of $\alpha_0$? Yes or no and show your work.   (Hint: read Theorem 14.1 from chapter 14)
	
	d. The sampling distribution of $\alpha_0^{mle}$ is complicated because the counts $x_i$ are dependent on each other as they have to sum to $n$. In this exercise, you will construct the distribution based on the following hypothetical data using  the non-parametric  bootstrap method.  \begin{table}[h]
	\centering
   \begin{tabular}{ccc}
    MM & FF & MF \\ \hline
    10 & 15 & 15\\ \hline
   \end{tabular}
   \end{table}

   Fill in the partial code provided  and echo it in the Appendix with clearly labeled section header. 
   
```{r label="boot_sim", include = FALSE, cache = TRUE}

### google "cache chunk option" to see what it does

set.seed(188)
x<-rep(c("MM","MF","FF"), times = c(15,15,10) )
B=10000

boot_sim <- lapply(1:B, FUN = function(i){ 
    # generate a resample from x #  
    
    #count the number of MM,MF, FF
    
    #calculate bootstrapped MLE of alpha and return value as dataframe
    
   })


boot_sim_mle <- do.call(rbind, boot_sim)


#write code to create a bootstrapped sampling distribution and save the #figure to an object called p1 and display it in part i. below by typing 
#p1 in a code chunk that is not echoed



#write code to calculate center of bootstrapped sampling distribution.
#save answer in variable and reference its value with inline code for 
#part ii. below


#write code to calculate bootstrap confidence intervals and reference the end points using inline code for part iii. below
```
  
   i. Make a histogram of the bootstrapped sampling distribution of $\widehat{\alpha}_0^{mle}$.  Don't forget those binwidths and label your axis/titles (using `expression` where necessary for math symbols. See Chapter 7 page 62 for a reminder ) 



   ii. Where is the bootstrapped sampling distribution centered? Does this make sense? Why?

   iii. Calculate and report (in context) a 95\% bootstrap confidence interval for $\alpha_0$. Calculate both types of intervals - standard and percentile - and report them in a neatly formatted table with headings and a title.


2. (Newton Raphson) Suppose $X \sim Geom(\pi_0)$, that is,
$$f(x) = (1- \pi_0)^{x} \: \pi_0 \ \ 0 \leq \pi_0 \leq 1$$

a. We need to find the root of the equation $s(\pi) = \frac{d}{d\pi} \ell(\pi) = 0$ in order to find the MLE of $\pi_0$. Write $s(\pi)$.

b. Say we decide to find the MLE of $\pi_0$ using Newton Raphson. Suppose we observe $x = 10$. Calculate $\pi_{new}$ assuming we begin the algorithm at $\pi_{old} = 0.5$. That is, perform one update of the Newton Raphson method.

3. (Batting averages) Recall that the beta distribution 
$$f(x) = \frac{\Gamma(\alpha_0+\beta_0)}{\Gamma(\alpha_0)\:\Gamma(\beta_0)}\:x^{\alpha_0-1} \: (1-x)^{\beta_0 - 1} \ \ 0 < x < 1$$
is a useful distribution for modeling proportions. The parameters $\alpha_0$ and $\beta_0$ are both required to be non-negative in order for $f(x)$ to be a valid PDF. 

Below are the batting averages for 16 randomly selected major league baseball players (from the 2015 season, minimum 200 at bats)



```{r}
ba <- c(0.276, 0.281, 0.225, 0.283, 0.257, 0.250, 0.250, 0.261, 0.312, 0.259, 0.273, 0.222, 0.314, 0.271, 0.294, 0.268)

   
```

a. Write the log-likelihood function $\ell(\alpha, \beta)$. Please leave the data as $x$'s and $n$ in your equation, do not plug in numbers. Don't forget the range for $\alpha$ and $\beta$.

b. Calculate $\widehat{\alpha}_0^{mom}$ and $\widehat{\beta}_0^{mom}$, the method of moments estimates of $\alpha_0$ and $\beta_0$. Show your code and also print the answers. (See problem 1 on Homework 5 from STAT 341 for the formulas for the M.O.M. estimators. You can find it in the homework sub-folder for STAT 342 )

c. We will now fit the beta distribution by maximum likelihood. 
Using the method of moments estimators as starting values for Newton Raphson, write code below to find the MLEs. (Show both code and output here)


d. Make a histogram of the batting average data along and overlay the fitted beta distribution from part c. (Show both code and output here)

4. (Bias/variance tradeoff) Suppose $X_1,X_2,\dots,X_n \overset{i.i.d.}{\sim} Norm(\mu_0, \sigma_0)$ where both parameters are unknown. 
Let
$S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n} (X_i - \bar{X})^2$ denote the usual sample variance. The maximum likelihood estimator is
$$\widehat{ \sigma^2}_{0}^{mle} = \frac{1}{n}\sum\limits_{i=1}^{n} (X_i - \bar{X})^2.$$

a. Recall the mean squared error (MSE) of an estimator is defined as 
$$\mbox{MSE} = \mbox{Bias}^2 + \mbox{Var}.$$
Write the mean squared error of $S^2$. (You do not need to prove results we have already proved in class or ones that you have proved on past homework. Just cite them with a reference.)

b. Find the MSE of $\widehat{\sigma^2}_0^{mle}$. (You do not need to prove results we have already proved in class or ones that you have proved on past homework. Just cite them with a reference.)

c. Make a plot of the ratio of the MSE of $\widehat{\sigma^2}_0^{mle}$ to the MSE of $S^2$ for $n$ from 1 to 100. Write a couple of sentences with your conclusion.


# Appendix

## Code for problem 1

