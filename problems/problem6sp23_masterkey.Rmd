---
title: "Problem Section 6"
subtitle:  "Likelihood Inference"
graphics: yes
output: pdf_document
header-includes: 
- \usepackage{amssymb, amsmath, amsfonts}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(maxLik)
library(fastR2)
```

### Exercises 

1. Suppose the number of students who arrive late to each of 10 consecutive lectures can be modeled as independent draws from a Poisson distribution with rate $\lambda_0$:
$$f(x) = \frac{ e^{-\lambda_0}\:\lambda_0^{x}}{x!} \ \ \ x = 0,1,2,\dots$$
The data are observed as follows: $x_1=0, x_2=0,x_3=0,x_4=1,x_5=1,x_6=1,x_7=2,x_8=2,x_9=3,x_10=3$.

a. Find the MLE $\widehat{\lambda}_0^{mle}$. (Hint: write the log likelihood function $\ell(\lambda)$ in terms of the $x$'s and only plug in the values after you have solved the first order equation. )

We have the likelihood as:

$$
L(\lambda_0) = \prod_{i=1}^n\frac{ e^{-\lambda_0}\:\lambda_0^{x_i}}{x_i!} = e^{-n\lambda_0}\lambda_0^{\sum_{i=1}^{10}x_i}\prod_{i=1}^n\frac{1}{x_i!}
$$

This gives us a log-likelihood of:

$$
\ell(\lambda_0) = -n\lambda_0 +\sum_{i=1}^nx_i\times ln(\lambda_0) -\sum_{i=1}^nln(x_i!)
$$

We have our first derivative as:

$$
\ell'(\lambda_0) = -n +\frac{\sum_{i=1}^nx_i}{\lambda_0}
$$
Setting this to zero and solving we thus yield:

$$
\hat{\lambda_0}^{MLE} = \frac{\sum_{i=1}^nx_i}{n} = \bar{x}
$$
This should make sense as a Poisson distribution is just an approximation of a binomial, and we saw that this was the same MLE for a binomial problem.

We have our second derivative as:

$$
\ell''(\lambda_0) = \frac{-\sum_{i=1}^nx_i}{\lambda_0^2} < 0
$$
So this is a maximum.

Plugging in our data we have this as:

```{r}
x <- c(0,0,0,1,1,1,2,2,3,3)
lambda_mle <- mean(x)
lambda_mle
```

b. Find the observed information $-\ell^{\prime \prime}(\widehat{\lambda}_0^{mle})$.

We know the observed information is $-\ell''(\lambda_0)$. Thus we have the observed information as:

$$
I(\lambda_0) = \frac{\sum_{i=1}^nx_i}{\lambda_0^2}
$$

c. Use the large sample normality of the MLE to find an approximate 95% Wald confidence interval for $\lambda_0$.

We have that the MLE will be asymptotically distributed as:

$$
\hat{\lambda_0}^{MLE} \sim N(\lambda_0,\sigma^2 =  I(\lambda_0)^{-1})
$$

Thus we have that:

$$
\hat{\lambda_0}^{MLE} \sim N(\lambda_0, \sigma^2 = \frac{\lambda_0^2}{\sum_{i=1}^nx_i})
$$

Thus we will have an approximate 95% Wald CI as:

$$
[\hat{\lambda_0}^{MLE} - qnorm(.975)\sqrt{\frac{\hat{\lambda_0}_{MLE}^2}{\sum_{i=1}^nx_i}},\hat{\lambda_0}^{MLE} + qnorm(.975)\sqrt{\frac{\hat{\lambda_0}_{MLE}^2}{\sum_{i=1}^nx_i}}]
$$

Plugging in our data we have this as:

```{r}
lower <- lambda_mle - qnorm(.975)*sqrt(lambda_mle^2/sum(x))
upper <- lambda_mle + qnorm(.975)*sqrt(lambda_mle^2/sum(x))
c(lower,upper)
```

d. Examine the quality of the second-order approximation to the log-likelihood. (I will come around and take a look at this on your computer) 

$$
\ell(\lambda_0) = -n\lambda_0 +\sum_{i=1}^nx_i\times ln(\lambda_0) -\sum_{i=1}^nln(x_i!)
$$

```{r}
loglik.pois <- function(lam){
  n <- length(x)
  -n*lam + log(lam)*sum(x) - sum(log(factorial(x)))
}

p1 <- maxLik2(loglik = loglik.pois,
start = 0.5,
method = "NR")

plot(p1) %>% gf_labs(title = "Second order Taylor Series Approximation")
```

e. Suppose we wish to test $H_0: \lambda_0 = \lambda_0^{null}$ versus $H_1: \lambda_0 \neq \lambda_0^{null}$. Give an expression for 
$$W = 2 \ln\left[ \frac{L(\widehat{\lambda}_0^{mle})}{L(\widehat{\lambda}_0^{null})} \right],$$ the likelihood ratio test statistic.

We will have the ratio as:

$$
\frac{e^{-n\widehat{\lambda}_0^{mle}}(\widehat{\lambda}_0^{mle})^{\sum_{i=1}^{10}x_i}\prod_{i=1}^n\frac{1}{x_i!}}{e^{-n(\widehat{\lambda}_0^{null})}(\widehat{\lambda}_0^{null})^{\sum_{i=1}^{10}x_i}\prod_{i=1}^n\frac{1}{x_i!}} = \frac{e^{-n\widehat{\lambda}_0^{mle}}(\widehat{\lambda}_0^{mle})^{\sum_{i=1}^{10}x_i}}{e^{-n(\widehat{\lambda}_0^{null})}(\widehat{\lambda}_0^{null})^{\sum_{i=1}^{10}x_i}}
$$

Taking the 2*ln of this gives us:

$$
2[-n(\widehat{\lambda}_0^{mle} -\widehat{\lambda}_0^{null}) + \sum_{i=1}^nx_i[ln(\widehat{\lambda}_0^{mle})-ln(\widehat{\lambda}_0^{mle})]]
$$

f. Calculate the (large sample) P-value testing $\lambda_0^{null}=1$. That is assume $W \sim \chi^2_1$ under the null hypothesis. 

```{r}
lik_mle <- sum(dpois(x,lambda_mle,log = TRUE))
lik_null <- sum(dpois(x,1,log=TRUE))
obs_W <- 2*(lik_mle-lik_null)
1-pchisq(obs_W,df=1)
```


g. Since $n=10$ is not really a large sample, an alternative approach is to calculate an empirical P-value. Take a look at chapter 20.3 where I show you how to calculate one for a uniform model. Write code below to find the empirical P-value for the likelihood ratio test statistic $W$ from part e.


```{r}
set.seed(175)
B <- 10000
sampling_function <- function(i){
  sample <- rpois(10,1)
  mle <- mean(sample)
  lik_mle <- sum(dpois(sample,mle,log = TRUE))
  lik_null <- sum(dpois(sample,1,log=TRUE))
  W <- 2*(lik_mle-lik_null)
  return(data.frame("test_stat" = W))
}
emp_samples <- lapply(1:B,sampling_function)
emp_samples_df <- do.call(rbind,emp_samples)
mean(emp_samples_df>=obs_W)
```



<!---
2. Suppose $X$ is a discrete random variable with PMF $f(x)$ indexed by a parameter $\theta$as shown below. 
\begin{table}[h]
\centering
\begin{tabular}{ccccc} 
& $x=1$ & $x=2$ & $x=3$ & $x=4$ \\ \hline
$\theta_0$ & 1/3 & 1/6 & 1/12 & 5/12 \\
$\theta_1$ & 1/2 & 1/4 & 1/6 & 1/12 \\ \hline \\ 
$W$ &  &   &  &  \\ \hline
\end{tabular}
\end{table}

a. Say we want to test $H_0:\theta=\theta_0^{null}$ versus $H_1:\theta \neq \theta_0^{null}$. Calculate the likelihood ratio statistic 
$$W= 2 \ln \left[ \frac{L(\widehat{\theta}_0^{mle})}{L(\theta_0^{null})} \right]$$ 
for for each value of $x$ and write it in the last row of the table. (Hint: The parameter $\theta$ can only take two values, so you can find the MLE fairly easily)

b. Write the sampling distribution of $W$ assuming $H_0$ is true. (Hint: $W$ is a discrete random variable) 

c. Suppose we observe $x=3$. Calculate the P-value. What should we conclude at a 0.05 level of significance?

--->